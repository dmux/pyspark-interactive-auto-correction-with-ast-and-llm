{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f580957-704a-43f2-a190-37df94228fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Resultados do bloco 1\n",
      "[OK] Fonte 'input_name' criada com formato 'PARQUET'.\n",
      "[OK] Código PySpark válido (sintaxe AST verificada).\n",
      "[OK] Código a ser executado 'input_name = spark.read.parquet('input_name.parquet')'.\n",
      "[OK] Código executado para 'input_name'.\n",
      "[OK] Código PySpark válido (sintaxe AST verificada).\n",
      "[OK] Código do metadata.pyspark.code executado.\n",
      "[OK] DataFrame 'df_saida' será retornado como pandas.DataFrame.\n",
      "[OK] Resultado de 'df_saida' convertido para pandas.DataFrame.\n",
      "[OK] Resultado final capturado do contexto como 'df_saida'.\n",
      "[OK] Tamanho: 3 linhas, 2 colunas\n",
      "[OK] Colunas: ['CONTRATO', 'VLVENINC']\n",
      "[OK] Tipo da coluna 'CONTRATO': int64\n",
      "[OK] Tipo da coluna 'VLVENINC': float64\n",
      "[OK] Os valores das células são idênticos\n",
      "\n",
      "📊 Resultado final:\n",
      "[{'CONTRATO': 67890, 'VLVENINC': 250000.0}, {'CONTRATO': 11123, 'VLVENINC': 300000.75}, {'CONTRATO': 78901, 'VLVENINC': 500000.25}]\n",
      "\n",
      "🔹 Resultados do bloco 2\n",
      "[OK] Fonte 'entrada_tabela' criada com formato 'DATAFRAME'.\n",
      "[OK] Código PySpark válido (sintaxe AST verificada).\n",
      "[OK] Código a ser executado 'entrada_tabela = spark.read.parquet('entrada_tabela.parquet')'.\n",
      "[OK] Código executado para 'entrada_tabela'.\n",
      "[OK] Código PySpark válido (sintaxe AST verificada).\n",
      "[OK] Código do metadata.pyspark.code executado.\n",
      "[OK] DataFrame 'df_resultado' salvo como TABELA.\n",
      "[OK] Resultado de 'df_resultado' convertido para pandas.DataFrame.\n",
      "[OK] Resultado final capturado do contexto como 'df_resultado'.\n",
      "[OK] Tamanho: 1 linhas, 2 colunas\n",
      "[OK] Colunas: ['ID', 'VALOR']\n",
      "[OK] Tipo da coluna 'ID': object\n",
      "[OK] Tipo da coluna 'VALOR': int64\n",
      "[OK] Os valores das células são idênticos\n",
      "\n",
      "📊 Resultado final:\n",
      "[{'ID': 'A2', 'VALOR': 20}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/zjs9h8rx6gj_2xc97nfw1f240000gn/T/ipykernel_38280/909155284.py:60: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "/var/folders/z4/zjs9h8rx6gj_2xc97nfw1f240000gn/T/ipykernel_38280/909155284.py:60: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class ParquetPathRewriter(ast.NodeTransformer):\n",
    "    def __init__(self, var_name):\n",
    "        self.var_name = var_name\n",
    "\n",
    "    def visit_Assign(self, node):\n",
    "        if (\n",
    "            isinstance(node.targets[0], ast.Name)\n",
    "            and node.targets[0].id == self.var_name\n",
    "            and isinstance(node.value, ast.Call)\n",
    "            and isinstance(node.value.func, ast.Attribute)\n",
    "            and node.value.func.attr in {\"parquet\", \"table\"}\n",
    "        ):\n",
    "            node.value.args = [ast.Constant(f\"{self.var_name}.parquet\")]\n",
    "        return node\n",
    "\n",
    "\n",
    "def rewrite_code_with_ast(code_str: str, var_name: str, fmt: str) -> object:\n",
    "    if fmt.upper() in {\"TABLE\", \"VIEW\"}:\n",
    "        return compile(code_str, filename=\"<raw>\", mode=\"exec\")\n",
    "    tree = ast.parse(code_str)\n",
    "    rewriter = ParquetPathRewriter(var_name)\n",
    "    new_tree = rewriter.visit(tree)\n",
    "    ast.fix_missing_locations(new_tree)\n",
    "    return compile(new_tree, filename=\"<ast>\", mode=\"exec\")\n",
    "\n",
    "\n",
    "def create_source_from_synthetic_data(spark, name, fmt, synthetic_data, exec_context):\n",
    "    if name not in synthetic_data:\n",
    "        raise ValueError(f\"O DataFrame synthetic_data['{name}'] não foi fornecido.\")\n",
    "\n",
    "    pdf = synthetic_data[name]\n",
    "\n",
    "    match fmt.upper():\n",
    "        case \"PARQUET\":\n",
    "            pdf.to_parquet(f\"{name}.parquet\", index=False)\n",
    "            df = spark.read.parquet(f\"{name}.parquet\")\n",
    "            df.createOrReplaceTempView(name)\n",
    "            exec_context[name] = df\n",
    "        case \"TABLE\":\n",
    "            df = spark.createDataFrame(pdf)\n",
    "            df.write.mode(\"overwrite\").saveAsTable(name)\n",
    "        case \"VIEW\":\n",
    "            df = spark.createDataFrame(pdf)\n",
    "            df.createOrReplaceTempView(name)\n",
    "        case \"DATAFRAME\":\n",
    "            exec_context[name] = spark.createDataFrame(pdf)\n",
    "        case _:\n",
    "            raise ValueError(f\"Formato de entrada '{fmt}' não suportado.\")\n",
    "\n",
    "\n",
    "def normalize_dataframe_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> list:\n",
    "    logs = []\n",
    "    try:\n",
    "        if df1.shape != df2.shape:\n",
    "            logs.append(f\"[FAIL] Tamanho diferente: esperado {df2.shape}, obtido {df1.shape}\")\n",
    "        else:\n",
    "            logs.append(f\"[OK] Tamanho: {df1.shape[0]} linhas, {df1.shape[1]} colunas\")\n",
    "\n",
    "        cols1 = list(df1.columns)\n",
    "        cols2 = list(df2.columns)\n",
    "        if cols1 != cols2:\n",
    "            logs.append(f\"[FAIL] Colunas diferentes: esperado {cols2}, obtido {cols1}\")\n",
    "        else:\n",
    "            logs.append(f\"[OK] Colunas: {cols1}\")\n",
    "\n",
    "        for col in df2.columns:\n",
    "            expected_type = df2[col].dtype\n",
    "            result_type = df1[col].dtype if col in df1.columns else None\n",
    "            if expected_type != result_type:\n",
    "                logs.append(f\"[FAIL] Tipo da coluna '{col}' difere: esperado {expected_type}, obtido {result_type}\")\n",
    "            else:\n",
    "                logs.append(f\"[OK] Tipo da coluna '{col}': {expected_type}\")\n",
    "\n",
    "        if not df1.equals(df2):\n",
    "            diff = (df1 != df2).sum().sum()\n",
    "            logs.append(f\"[FAIL] Valores diferentes: {diff} célula(s) divergentes\")\n",
    "        else:\n",
    "            logs.append(f\"[OK] Os valores das células são idênticos\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logs.append(f\"[ERROR] Erro ao comparar DataFrames: {e}\")\n",
    "    return logs\n",
    "\n",
    "\n",
    "def validate_syntax(code: str) -> str:\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return \"[OK] Código PySpark válido (sintaxe AST verificada).\"\n",
    "    except SyntaxError as e:\n",
    "        return f\"[ERROR] Código PySpark inválido: {e}\"\n",
    "\n",
    "\n",
    "def execute_pyspark_block(payload: dict, synthetic_data: dict, expected_result: pd.DataFrame = None):\n",
    "    spark = SparkSession.builder.appName(\"Executor\").getOrCreate()\n",
    "    exec_context = {\"spark\": spark}\n",
    "    logs = []\n",
    "    block_id = payload.get(\"block_id\", \"unknown\")\n",
    "    input_data = payload.get(\"input_data\", [])\n",
    "    output_data = payload.get(\"output_data\", [])\n",
    "    metadata_code = payload.get(\"metadata\", {}).get(\"pyspark\", {}).get(\"code\", \"\")\n",
    "\n",
    "    try:\n",
    "        if not input_data:\n",
    "            raise ValueError(\"Payload precisa conter ao menos um 'input_data'.\")\n",
    "        if not output_data:\n",
    "            raise ValueError(\"Payload precisa conter ao menos um 'output_data'.\")\n",
    "\n",
    "        for entry in input_data:\n",
    "            info = entry.get(\"pyspark_data\", {})\n",
    "            name = info.get(\"name\")\n",
    "            fmt = info.get(\"format\", \"PARQUET\")\n",
    "            create_source_from_synthetic_data(spark, name, fmt, synthetic_data, exec_context)\n",
    "            logs.append(f\"[OK] Fonte '{name}' criada com formato '{fmt}'.\")\n",
    "\n",
    "        for entry in input_data:\n",
    "            info = entry[\"pyspark_data\"]\n",
    "            name = info[\"name\"]\n",
    "            code = entry.get(\"pyspark_code\")\n",
    "            if not code:\n",
    "                fmt = info.get(\"format\", \"PARQUET\").upper()\n",
    "                if fmt == \"TABLE\":\n",
    "                    code = f\"{name} = spark.read.table('{name}')\"\n",
    "                elif fmt == \"VIEW\":\n",
    "                    code = f\"{name} = spark.sql('SELECT * FROM {name}')\"\n",
    "                else:\n",
    "                    code = f\"{name} = spark.read.parquet('{name}.parquet')\"\n",
    "            logs.append(validate_syntax(code))\n",
    "            compiled = rewrite_code_with_ast(code, name, info.get(\"format\", \"PARQUET\"))\n",
    "            logs.append(f\"[OK] Código a ser executado '{code}'.\")\n",
    "            exec(compiled, exec_context)\n",
    "            logs.append(f\"[OK] Código executado para '{name}'.\")\n",
    "\n",
    "        if metadata_code:\n",
    "            logs.append(validate_syntax(metadata_code))\n",
    "            exec(metadata_code, exec_context)\n",
    "            logs.append(f\"[OK] Código do metadata.pyspark.code executado.\")\n",
    "\n",
    "        for entry in output_data:\n",
    "            info = entry[\"pyspark_data\"]\n",
    "            name = info[\"name\"]\n",
    "            fmt = info.get(\"format\", \"VIEW\").upper()\n",
    "\n",
    "            if name not in exec_context:\n",
    "                raise ValueError(f\"DataFrame '{name}' não encontrado após execução.\")\n",
    "\n",
    "            df = exec_context[name]\n",
    "\n",
    "            match fmt:\n",
    "                case \"VIEW\":\n",
    "                    df.createOrReplaceTempView(name)\n",
    "                    logs.append(f\"[OK] DataFrame '{name}' registrado como VIEW.\")\n",
    "                case \"TABLE\":\n",
    "                    df.write.mode(\"overwrite\").saveAsTable(name)\n",
    "                    logs.append(f\"[OK] DataFrame '{name}' salvo como TABELA.\")\n",
    "                case \"DATAFRAME\":\n",
    "                    logs.append(f\"[OK] DataFrame '{name}' será retornado como pandas.DataFrame.\")\n",
    "                case \"PARQUET\":\n",
    "                    df.toPandas().to_parquet(f\"{name}_output.parquet\", index=False)\n",
    "                    logs.append(f\"[OK] DataFrame '{name}' salvo como arquivo Parquet.\")\n",
    "                case _:\n",
    "                    raise ValueError(f\"Formato de saída '{fmt}' não é suportado.\")\n",
    "\n",
    "            try:\n",
    "                info[\"result\"] = df.toPandas().to_dict(orient=\"records\")\n",
    "                logs.append(f\"[OK] Resultado de '{name}' convertido para pandas.DataFrame.\")\n",
    "            except Exception as e:\n",
    "                logs.append(f\"[ERROR] Conversão para pandas falhou para '{name}': {e}\")\n",
    "                info[\"result\"] = f\"Erro: {e}\"\n",
    "\n",
    "        metadata_result = None\n",
    "        if expected_result is not None:\n",
    "            for entry in output_data:\n",
    "                output_name = entry[\"pyspark_data\"][\"name\"]\n",
    "                if output_name in exec_context:\n",
    "                    result_df = exec_context[output_name]\n",
    "                    if hasattr(result_df, \"toPandas\"):\n",
    "                        logs.append(f\"[OK] Resultado final capturado do contexto como '{output_name}'.\")\n",
    "                        pandas_result = normalize_dataframe_types(result_df.toPandas())\n",
    "                        expected_result = normalize_dataframe_types(expected_result)\n",
    "                        comparison_logs = compare_dataframes(pandas_result, expected_result)\n",
    "                        logs.extend(comparison_logs)\n",
    "                        metadata_result = pandas_result.to_dict(orient=\"records\")\n",
    "                        break\n",
    "            if metadata_result is None:\n",
    "                logs.append(\"[FAIL] Nenhum resultado correspondente ao output_data encontrado no contexto para comparar.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logs.append(f\"[ERROR] Falha no bloco '{block_id}': {e}\")\n",
    "        logs.append(traceback.format_exc())\n",
    "        metadata_result = None\n",
    "\n",
    "    return {\n",
    "        \"block_id\": block_id,\n",
    "        \"logs\": logs,\n",
    "        \"output_data\": output_data,\n",
    "        \"metadata_result\": metadata_result\n",
    "    }\n",
    "\n",
    "\n",
    "def execute_multiple_blocks(payloads: list, expected_results: dict):\n",
    "    all_results = []\n",
    "    for payload in payloads:\n",
    "        block_id = payload.get(\"block_id\")\n",
    "        synthetic_data = payload.get(\"metadata\", {}).get(\"synthetic_data\", {})\n",
    "        expected = expected_results.get(block_id)\n",
    "        result = execute_pyspark_block(payload, synthetic_data, expected)\n",
    "        all_results.append(result)\n",
    "    return all_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    payloads = [\n",
    "        {\n",
    "            \"block_id\": \"1\",\n",
    "            \"metadata\": {\n",
    "                \"pyspark\": {\n",
    "                    \"code\": \"df_saida = spark.sql('SELECT * FROM input_name WHERE CAST(VLVENINC AS DOUBLE) > 200000')\"\n",
    "                },\n",
    "                \"synthetic_data\": {\n",
    "                    \"input_name\": pd.DataFrame([\n",
    "                        [\"12345\", \"100000.50\"],\n",
    "                        [\"67890\", \"250000.00\"],\n",
    "                        [\"11123\", \"300000.75\"],\n",
    "                        [\"44455\", \"150000.00\"],\n",
    "                        [\"78901\", \"500000.25\"]\n",
    "                    ], columns=[\"CONTRATO\", \"VLVENINC\"])\n",
    "                }\n",
    "            },\n",
    "            \"input_data\": [\n",
    "                {\n",
    "                    \"pyspark_data\": {\n",
    "                        \"name\": \"input_name\",\n",
    "                        \"format\": \"PARQUET\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"output_data\": [\n",
    "                {\n",
    "                    \"pyspark_data\": {\n",
    "                        \"name\": \"df_saida\",\n",
    "                        \"format\": \"DATAFRAME\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"block_id\": \"2\",\n",
    "            \"metadata\": {\n",
    "                \"pyspark\": {\n",
    "                    \"code\": \"df_resultado = spark.sql('SELECT * FROM entrada_tabela WHERE VALOR > 15')\"\n",
    "                },\n",
    "                \"synthetic_data\": {\n",
    "                    \"entrada_tabela\": pd.DataFrame([\n",
    "                        [\"A1\", 10],\n",
    "                        [\"A2\", 20]\n",
    "                    ], columns=[\"ID\", \"VALOR\"])\n",
    "                }\n",
    "            },\n",
    "            \"input_data\": [\n",
    "                {\n",
    "                    \"pyspark_data\": {\n",
    "                        \"name\": \"entrada_tabela\",\n",
    "                        \"format\": \"DATAFRAME\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"output_data\": [\n",
    "                {\n",
    "                    \"pyspark_data\": {\n",
    "                        \"name\": \"df_resultado\",\n",
    "                        \"format\": \"TABLE\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    expected_results = {\n",
    "        \"1\": pd.DataFrame([\n",
    "            [\"67890\", 250000.00],\n",
    "            [\"11123\", 300000.75],\n",
    "            [\"78901\", 500000.25]\n",
    "        ], columns=[\"CONTRATO\", \"VLVENINC\"]),\n",
    "        \"2\": pd.DataFrame([\n",
    "            [\"A2\", 20]\n",
    "        ], columns=[\"ID\", \"VALOR\"])\n",
    "    }\n",
    "    expected_results[\"1\"][\"CONTRATO\"] = expected_results[\"1\"][\"CONTRATO\"].astype(str)\n",
    "    expected_results[\"1\"][\"VLVENINC\"] = expected_results[\"1\"][\"VLVENINC\"].astype(float)\n",
    "    expected_results[\"2\"][\"ID\"] = expected_results[\"2\"][\"ID\"].astype(str)\n",
    "    expected_results[\"2\"][\"VALOR\"] = expected_results[\"2\"][\"VALOR\"].astype(int)\n",
    "\n",
    "    results = execute_multiple_blocks(payloads, expected_results)\n",
    "\n",
    "    for result in results:\n",
    "        print(f\"\\n🔹 Resultados do bloco {result['block_id']}\")\n",
    "        for log in result[\"logs\"]:\n",
    "            print(log)\n",
    "        print(\"\\n📊 Resultado final:\")\n",
    "        print(result[\"metadata_result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36ede9-af35-4fc7-be76-6c63262e6800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
